{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "from train_autoencoder import smile_convert\n",
    "from sample_autoencoder import load_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pre-trained weights and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_file = \"../data/best_vae_model.json\"\n",
    "weights_file = \"../data/best_vae_annealed_weights.h5\"\n",
    "char_file = \"../data/zinc_char_list.json\"\n",
    "test_file = \"../data/250k_rndm_zinc_drugs_clean.smi\"\n",
    "limit = 20 ## number of test data points to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = json.load(open(char_file))\n",
    "model_dict = json.load(open(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The saved model file includes both the encoder and decoder in a single Sequential model.\n",
    "\n",
    "We're going to split this out into separate encoder and decoder models.\n",
    "\n",
    "This involves first locating the `VariationalDense` layer which generates the mean and variance of the encoding, and then setting up an encoder model which just runs through the first layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae_layer = np.argmax([L[\"name\"] == \"VariationalDense\" for L in model_dict[\"layers\"]])\n",
    "\n",
    "encoder = model_dict.copy()\n",
    "del encoder['loss']\n",
    "del encoder['optimizer']\n",
    "encoder['layers'] = encoder['layers'][:vae_layer+1]\n",
    "max_len = encoder[\"layers\"][0][\"batch_input_shape\"][1]\n",
    "n_chars = encoder[\"layers\"][0][\"batch_input_shape\"][2]\n",
    "encoder = model_from_json(json.dumps(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting weights for layer 0 8\n",
      "Weights for this layer have shapes [(9, 35, 9, 1), (9,)]\n",
      "setting weights for layer 1 8\n",
      "Weights for this layer have shapes [(9,), (9,), (9,), (9,)]\n",
      "setting weights for layer 2 8\n",
      "Weights for this layer have shapes [(9, 9, 9, 1), (9,)]\n",
      "setting weights for layer 3 8\n",
      "Weights for this layer have shapes [(9,), (9,), (9,), (9,)]\n",
      "setting weights for layer 4 8\n",
      "Weights for this layer have shapes [(10, 9, 11, 1), (10,)]\n",
      "setting weights for layer 5 8\n",
      "Weights for this layer have shapes [(10,), (10,), (10,), (10,)]\n",
      "setting weights for layer 6 8\n",
      "Weights for this layer have shapes []\n",
      "setting weights for layer 7 8\n",
      "Weights for this layer have shapes [(940, 435), (435,)]\n",
      "setting weights for layer 8 8\n",
      "Weights for this layer have shapes [(435, 292), (292,), (435, 292), (292,)]\n"
     ]
    }
   ],
   "source": [
    "def set_encoder_weights(weights_file, model, vae_layer):\n",
    "    with h5py.File(weights_file, mode='r') as fp:\n",
    "        for k in range(min(fp.attrs['nb_layers'], vae_layer+1)):\n",
    "            print \"setting weights for layer\", k, vae_layer\n",
    "            g = fp['layer_{}'.format(k)]\n",
    "            weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "            w_shape = [i.shape for i in weights]\n",
    "            print('Weights for this layer have shapes {}'.format(w_shape))\n",
    "            try:\n",
    "                model.layers[k].set_weights(weights)\n",
    "            except AssertionError:\n",
    "                print('Failed loading weights on layer {}. '\n",
    "                                   'Weights initiated with random'.format(k))\n",
    "                continue\n",
    "                \n",
    "set_encoder_weights(weights_file, encoder, vae_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "For the decoder to work, we also need to add the ability to take an input directly, since it is no longer attached to a preceding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder = model_dict.copy()\n",
    "decoder['layers'] = decoder['layers'][vae_layer+1:]\n",
    "decoder['layers'][0]['batch_input_shape'] = encoder.output_shape\n",
    "del decoder['optimizer'] ## Why is this necessary? This is a problem.\n",
    "decoder = model_from_json(json.dumps(decoder))\n",
    "decoder.sample_weight_mode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting weights for layer 9 0 8\n",
      "Weights for this layer have shapes [(292,), (292,), (292,), (292,)]\n",
      "setting weights for layer 10 1 8\n",
      "Weights for this layer have shapes [(292, 292), (292,)]\n",
      "setting weights for layer 11 2 8\n",
      "Weights for this layer have shapes []\n",
      "setting weights for layer 12 3 8\n",
      "Weights for this layer have shapes [(292, 501), (501, 501), (501,), (292, 501), (501, 501), (501,), (292, 501), (501, 501), (501,)]\n",
      "setting weights for layer 13 4 8\n",
      "Weights for this layer have shapes [(501, 501), (501, 501), (501,), (501, 501), (501, 501), (501,), (501, 501), (501, 501), (501,)]\n",
      "setting weights for layer 14 5 8\n",
      "Weights for this layer have shapes [(501, 501), (501, 501), (501,), (501, 501), (501, 501), (501,), (501, 501), (501, 501), (501,)]\n",
      "setting weights for layer 15 6 8\n",
      "Weights for this layer have shapes [(501, 35), (35, 35), (35,), (501, 35), (35, 35), (35,), (501, 35), (35, 35), (35,), (35, 35)]\n"
     ]
    }
   ],
   "source": [
    "def set_decoder_weights(weights_file, model, vae_layer):\n",
    "    with h5py.File(weights_file, mode='r') as fp:\n",
    "        for k in range(vae_layer+1, fp.attrs['nb_layers']):\n",
    "            decoder_ix = k - vae_layer - 1\n",
    "            print \"setting weights for layer\", k, decoder_ix, vae_layer\n",
    "            g = fp['layer_{}'.format(k)]\n",
    "            weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "            w_shape = [i.shape for i in weights]\n",
    "            print('Weights for this layer have shapes {}'.format(w_shape))\n",
    "            try:\n",
    "                model.layers[decoder_ix].set_weights(weights)\n",
    "            except AssertionError:\n",
    "                print('Failed loading weights on layer {}. '\n",
    "                                   'Weights initiated with random'.format(k))\n",
    "                continue\n",
    "                \n",
    "set_decoder_weights(weights_file, decoder, vae_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set size is', 500)\n",
      "Training set size is 500, after filtering to max length of 120\n",
      "('total chars:', 35)\n"
     ]
    }
   ],
   "source": [
    "test_set = load_test_data(test_file, n_chars, max_len, char_list, limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_smiles(one_hot, truncate=True):\n",
    "    \"\"\" If `truncate=True`, stop at first space. If `False`, don't remove internal spaces\"\"\"\n",
    "    char_array = np.array(char_list)[np.argmax(one_hot, 2)]\n",
    "    if truncate:\n",
    "        return map(lambda x: ''.join(x).split(' ',1)[0], char_array)\n",
    "    else:\n",
    "        return map(lambda x: ''.join(x).strip(), char_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import function\n",
    "\n",
    "seq_input = encoder.get_first_input()\n",
    "encode = function([seq_input], encoder.layers[-1].get_mean_logsigma(encoder.layers[-2].get_output()))\n",
    "\n",
    "enc_input = decoder.get_first_input()\n",
    "output = decoder.get_output()\n",
    "decode = function([enc_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = 20\n",
    "\n",
    "mu, logsigma = encode(test_set[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CCOC(=O)C[C@H](C)CNC(=O)C(=O)N1CCc2ccc(C)cc21',\n",
       " u'ClC(Cl)(Cl)c1nonc1C(Cl)(Cl)Cl',\n",
       " u'CCc1cccc(CC)c1NC(=O)NC1CC1',\n",
       " u'Cc1ccc(C)c(NC(=S)NCCc2cccs2)c1',\n",
       " u'Cc1nnsc1C(=O)Nc1nnc(-c2ccc(Br)cc2)o1',\n",
       " u'COc1ccc(C(=O)N(C)[C@@H](C)C/C(N)=N/O)cc1O',\n",
       " u'CC(C)c1nsc(NC[C@H](C2CC2)[NH+](C)C)n1',\n",
       " u'O=C(Nc1ccc(Oc2ccc(Cl)nn2)cc1)[C@@H](O)c1ccccc1',\n",
       " u'CCCCn1nc(C)c(C[NH2+]C[C@@H](C)O)c1Cl',\n",
       " u'COC(=O)c1ccc(NC(=O)c2c(C)sc3ncnc(N4CCC[C@H](C)C4)c23)cc1',\n",
       " u'CCn1cc(/C=C/C(=O)c2ccc3ccccc3c2)cn1',\n",
       " u'CCc1nc2n(n1)CCC[C@H]2NC(=O)c1ccc(-n2cc(C)cn2)cc1',\n",
       " u'NC(=O)c1ccc(NC(=O)c2cccn(Cc3ccc(F)cc3)c2=O)cc1',\n",
       " u'Cc1ccc(-c2nc(C[NH+]3CCCC[C@H]3c3cccnc3)c(C)o2)s1',\n",
       " u'COc1ccc(OC)c(S(=O)(=O)n2cc3c(=O)n(C)c(=O)n(C)c3n2)c1',\n",
       " u'O=C1/C(=C/c2ccccc2)Oc2c1ccc1c2CN(Cc2cccs2)CO1',\n",
       " u'CC[NH+](CC)[C@](C)(CC)[C@H](O)c1cscc1Br',\n",
       " u'Cc1noc(C)c1CCCNC(=O)N[C@H]1CC(=O)N(C2CC2)C1',\n",
       " u'CCCNC(=O)[C@H]1CS[C@H](c2ccccc2O)N1C(C)=O',\n",
       " u'CC[C@H](NC(=O)NCc1c(C)noc1C)c1ccc(OC)cc1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_smiles(test_set[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_hat = decode(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CCOC(=O)N[C@H](C)CNC(=O)C(=O)N CCc2ccc(C)cc21',\n",
       " u'CC (CN C(C)c1n nc1C(',\n",
       " u'CC 1cccc(C )c1NC(=O)NC1CC1',\n",
       " u'Cc1ccc( )c(NC(=O) CCc2cccs2)c1',\n",
       " u'Cc1nncc1C(=O)Nc1ccn(-c2ccc(Br)cc2)n1',\n",
       " u'COc1ccc(C(=O)N(C)[C@@H](C)CNC(C)(N)O',\n",
       " u'CC(C)c1nnc(NC C@H](CCC   [C@H](C)C)n1',\n",
       " u'O=C(Nc1ccc(-c2ccc(Cl)n 2)cc1)[C@@H](O)c1ccccc1',\n",
       " u'C#CC  nc(C)c(C[NH2+]C[C@@H](C)C',\n",
       " u'COC(=O)c1ccc(NC(=O)c2c(C)nc3ccnc(N3CCC[C@H](C)C',\n",
       " u'CC 1cc(/C= /C(=O) 2ccc3ccccc3c2)cn1',\n",
       " u'CC 1nc2n(n1)CCC[C@H] NC(=O)c1ccc(-n2cc(C)cc2)cc1',\n",
       " u'CC(=O)c1ccc(NC(=O)c2cccc(Nc3ccc(F)cc3)c2=O)cc1',\n",
       " u'Cc1ccc(-c2nc(C[NH+]3CCC [C@H]3c3ccc c3)c(C)n2)n1',\n",
       " u'COc1ccc(OC)c(S(=O)(=O)c2cc3c(   n    (    (C)c3n2)c1',\n",
       " u'O=C1/C(=C/c2ccccc2)O c     2c1N(C(c  c sc',\n",
       " u'CC[NH+](CC)[C@](C)(CC [C@H]( )c1cccc1Br',\n",
       " u'Cc1noc(C)c1CCCNC(=O)N[C@H]1CC(=O)N(C2CC2)C1',\n",
       " u'CCCN( =O)[C@H]1CC[C@H](c2ccccc1O)',\n",
       " u'CC[C@H](NC(=O)NCc1c(C)n c1C)c1ccc(OC)cc1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_smiles(x_hat, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94499999999999995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(x_hat, 2) == np.argmax(test_set[:x_hat.shape[0]], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we sample $z$, instead of looking just at the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior = norm(np.zeros(mu.shape), np.ones(mu.shape))\n",
    "q_dist = norm(mu, np.exp(logsigma))\n",
    "sampled = q_dist.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87541666666666662"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = decode(np.array(sampled, dtype=np.float32))\n",
    "np.mean(np.argmax(x_hat, 2) == np.argmax(test_set[:x_hat.shape[0]], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-46.62651038, -44.5470448 , -33.32653342, -45.47039925,\n",
       "       -55.72006952, -34.23518116, -41.84344367, -58.49255424,\n",
       "       -39.90348387, -57.36332324, -35.98827463, -58.83815762,\n",
       "       -32.73888906, -61.64702754, -63.63639275, -61.20123432,\n",
       "       -58.01327165, -44.96964335, -48.96139468, -42.20242053])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prior.logpdf(sampled) - q_dist.logpdf(sampled)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CCOC(=O)[C@@H](C)CC[C@@H](=O)N1CC1c1cc(F)cc1',\n",
       " u'OCC(N)(C#N)c1n n',\n",
       " u'COc1cccc(C )c1NC(=O)N',\n",
       " u'Cc1ccc( )c(NC(=O)NC c2cccc2)c1',\n",
       " u'C 1nccc1C(=O)Nc1cnc(-c2cc (Br)cc2) o1',\n",
       " u'Cc1(cc( C(=O)N(C)[C@@H](C)C2C( )(F)',\n",
       " u'CN CO 1ncc(N CC23(C   3) [NH3+])(C)n1',\n",
       " u'O=C(  ccc O c2cc (Cl)n    C1)[C@@H](O)c1ccccc1',\n",
       " u'CCCc1cnc( NC(C[NH2+]C[C@@H](C)C)c1',\n",
       " u'COC(=O)c1ccc(NC =O)c2c(C)nc3cn ( N2CCC4C4)c(C)c2)cc1',\n",
       " u'CCn1c ( (=S/C(=O)c2c (C)c',\n",
       " u'CCc1cc2n(n1)CCC[C@H]2NC(=O)    c(- 2n (C)c',\n",
       " u'O=C(NOc1ccc(NC =O)Nc ccc Nc2cc(F)ccc2)c2=O)c1',\n",
       " u'Cc1ccc(-c2nc(C[NH+]2CC (C(=O)Cc3cccc 4) ([no2)c1',\n",
       " u'COc1cc(OC )c(S(=O)(=O)c2c 3c(=O) (C)c(=O)c(C)c3',\n",
       " u'O=C(NC(C#Nc2cccc 2 OCc1F  (F)N2(CCc1cccs',\n",
       " u'CC[ @H](CC)[C@H](C(C  [C@@H])Cc1cc',\n",
       " u'Clc1nc(C)c1CCCNC(=O)N[C@H]1CC(=O)N(C2CC2)C',\n",
       " u'C CCC(=O)[C@H]1C[C@@H](c2ccccc2Br) S1)[',\n",
       " u'C [C@H](NC(=O)NC NC(C)ccc 1)c1ccc(Br)c(C)c1']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_smiles(x_hat, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CCOC(=O)[C@@H](C)CC[C@@H](=O)N1CC1c1cc(F)cc1',\n",
       " u'OCC(N)(C#N)c1n',\n",
       " u'COc1cccc(C',\n",
       " u'Cc1ccc(',\n",
       " u'C',\n",
       " u'Cc1(cc(',\n",
       " u'CN',\n",
       " u'O=C(',\n",
       " u'CCCc1cnc(',\n",
       " u'COC(=O)c1ccc(NC',\n",
       " u'CCn1c',\n",
       " u'CCc1cc2n(n1)CCC[C@H]2NC(=O)',\n",
       " u'O=C(NOc1ccc(NC',\n",
       " u'Cc1ccc(-c2nc(C[NH+]2CC',\n",
       " u'COc1cc(OC',\n",
       " u'O=C(NC(C#Nc2cccc',\n",
       " u'CC[',\n",
       " u'Clc1nc(C)c1CCCNC(=O)N[C@H]1CC(=O)N(C2CC2)C',\n",
       " u'C',\n",
       " u'C']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_smiles(x_hat, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
